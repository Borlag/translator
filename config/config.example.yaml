# docxru example configuration

llm:
  provider: mock          # mock | openai | google | ollama
  model: gpt-4o-mini      # openai/ollama model; ignored by google
  source_lang: en         # used by google provider
  target_lang: ru         # used by google provider
  base_url: null          # openai: custom API URL; google: endpoint base; ollama: e.g. http://localhost:11434
  system_prompt_path: null # optional .md/.txt with extra translation instructions
  glossary_path: null      # optional .md/.txt glossary file with EN->RU pairs
  glossary_in_prompt: true # false = do not send full glossary in every prompt
  hard_glossary: false     # true = adaptive term locking via placeholders (TOC/table/short labels); can still hurt RU morphology in prose
  reasoning_effort: null   # openai hint: none|minimal|low|medium|high|xhigh
  prompt_cache_key: null   # optional openai cache key for repeated prompt prefixes
  prompt_cache_retention: null # optional openai retention hint, e.g. "24h"
  structured_output_mode: "auto" # off | auto | strict
  temperature: 0.0
  max_output_tokens: 2000
  retries: 2
  timeout_s: 60.0
  prompt_examples_mode: "core" # off | core
  glossary_prompt_mode: "full" # off | full | matched
  glossary_match_limit: 24
  batch_segments: 1      # >1 enables grouped translation (OpenAI/Ollama) for better context continuity
  batch_max_chars: 12000 # soft payload cap per grouped request
  batch_skip_on_brline: true
  batch_max_style_tokens: 16
  batch_tm_hints_per_item: 1
  batch_recent_translations_per_item: 3
  domain_term_pairs_path: config/domain_term_pairs.yaml
  context_window_chars: 900 # >0 disables grouped/parallel translate and injects recent EN=>RU context
  auto_model_sizing: false # true = auto-tune batch/checker request sizes by model context limits

tm:
  path: translation_cache.sqlite
  fuzzy_enabled: false
  fuzzy_top_k: 3
  fuzzy_min_similarity: 0.75
  fuzzy_prompt_max_chars: 500
  fuzzy_token_regex: "[A-Za-zА-Яа-яЁё0-9]{2,}"
  fuzzy_rank_mode: "hybrid" # sequence | hybrid

pdf:
  bilingual_mode: false
  ocr_fallback: false
  max_pages: null
  max_font_shrink_ratio: 0.6
  block_merge_threshold_pt: 12.0
  skip_headers_footers: false
  table_detection: true
  font_map: {}
  default_sans_font: NotoSans-Regular.ttf
  default_serif_font: NotoSerif-Regular.ttf
  default_mono_font: NotoSansMono-Regular.ttf

checker:
  enabled: false
  provider: null           # null = reuse llm.provider
  model: gpt-5-mini        # null = reuse llm.model; gpt-5-mini is recommended for higher-quality checker edits
  temperature: 0.0
  max_output_tokens: 6000  # higher default helps avoid checker finish_reason=length on large chunks
  timeout_s: 60.0
  retries: 0
  system_prompt_path: null # optional checker-specific instructions
  glossary_path: null      # optional checker-specific glossary file (not required for chunk mode)
  pages_per_chunk: 3       # check translation by 3-page windows when page numbers are available
  fallback_segments_per_chunk: 80 # used when true page numbers are unavailable (DOCX)
  max_segments: 0          # 0 = no limit
  only_on_issue_severities: ["warn", "error"] # if empty, checker audits all translated segments
  only_on_issue_codes: []  # optional deterministic issue-code filter
  output_path: checker_suggestions.json
  safe_output_path: checker_suggestions_safe.json # auto-filtered no-op/token-safe subset
  auto_apply_safe: false   # true = auto-apply safe checker edits into output DOCX
  auto_apply_min_confidence: 0.7 # safe auto-apply confidence floor [0..1]
  openai_batch_enabled: false # async overnight checker via OpenAI Batch API (requires openai checker provider)
  openai_batch_completion_window: "24h"
  openai_batch_poll_interval_s: 20.0
  openai_batch_timeout_s: 86400.0

pricing:
  enabled: false
  pricing_path: null       # YAML/JSON map: provider -> model -> input/output_per_million
  currency: USD

run:
  run_dir: null            # null = output folder; else a base dir where run_id subfolders are created
  run_id: null             # null = auto UTC timestamp
  status_path: null        # null = <run_dir>/run_status.json
  dashboard_html_path: null # null = <run_dir>/dashboard.html
  status_flush_every_n_segments: 10
  batch_timeout_bisect: true # on grouped timeout split batch recursively instead of retrying same size
  fail_fast_on_translate_error: true # true = stop translation immediately on LLM translate errors

include_headers: false
include_footers: false

concurrency: 4
mode: reflow              # reflow | com
abbyy_profile: "off"        # off | safe | aggressive
glossary_lemma_check: "off" # off | warn | retry
short_translation_min_ratio: 0.35
short_translation_min_source_chars: 24
untranslated_latin_warn_ratio: 0.15
untranslated_latin_min_len: 3
untranslated_latin_allowlist_path: null
repeated_words_check: true
repeated_phrase_ngram_max: 3
context_leakage_check: true
context_leakage_allowlist_path: null
layout_check: false         # run layout risk checks after translation
layout_expansion_warn_ratio: 1.5
layout_auto_fix: false      # apply optional spacing/font fixes for risky segments
layout_font_reduction_pt: 0.5
layout_spacing_factor: 0.8

qa_report_path: qa_report.html
qa_jsonl_path: qa.jsonl
translation_history_path: null # optional append-only jsonl with source/target/context per successful segment
log_path: run.log

patterns:
  preset_file: regex_presets.yaml
  preset_name: default
